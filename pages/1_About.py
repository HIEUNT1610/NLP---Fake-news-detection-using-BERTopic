import streamlit as st

# Title and Layout:
st.set_page_config(page_title="Fake news detection using BERTopic", layout="wide")
st.title("Fake news detection using BERTopic")
st.header("""
            A simple demo on fake news detection using topic modelling.""")

st.markdown("""        
            About the demo app:
            
            This demo app is built to tackle the problem of fake news detection using topic modelling. The idea is that we can detect fake news by comparing the topic of a given document or a news article, then compare it to the topics generated by the two BERTopic models using the two datasets. If the topic of the input document is similar to the topics in the fake news dataset, the input document is likely to have a common fake news topic, and therefore is likely warrant a further assessment of veracity. This is by no means a perfect solution to detect fake news, but it can be a good way to quickly skim through a large number of documents and articles to find the ones that are most likely to be fake.
            
            The backbone of the demo is BERTopic, a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.
            
            The models used in the demo were trained on Misinfo dataset from Kaggle, based on EUvsDisinfo data, using miniLM Sentence Transformer embedding. This application seeks to combine unsupervised learning techniques and labelled data to detect fake news. Two BERTopic models were used to model topics based on the fake and true news datasets. Prediction is done by generating the topic of the input and comparing to the topics clustered by the model based on the two datasets. If the input topic is similar to the topics in the fake news dataset, the input is detected to be in common fake news topics.                      
            
            At the moment, the accuracy in prediction of this demo app is not too high due to the limited hosting space from Streamlit and training data, but it is showing promise. Better and larger embedding models (such as paraphrase-multilingual-mpnet-base-v2) can give much better detection, but it goes over the limit of Streamlit Community Cloud. Performance can also be further improved upon by continually adding more data to the training set. 
            """)

# Some other things to write in here, such as what is BERTopic and how it works, how the models were trained, etc.
st.subheader("About BERTopic:")
st.markdown("""
            BERTopic is an impressive tool developed by Maarten Grootendorst (2022). It combines the strengths of BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. Grootendorst also built BERTopic in a modular manner, allowing easy customization and integration with other NLP tools. Grootendorst's principle is interesting because it may be possible to modify this pipeline to other types of data such as images, sounds, etc.            
            
            BERTopic can be found in this link from Grootendorst's Github: https://maartengr.github.io/BERTopic/index.html
            """)

st.subheader("About the dataset:")
st.markdown("""
            The MisInfo dataset is a 79k news dataset from Kaggle, based on EUvsDisinfo data. It contains 43642 misinfo, fake news or propaganda articles and 34975 'true' news articles. The structure of the dataset made it easy to train classification models, however topic modelling is an interesting approach towards the problem of fake news detection.
            
            The dataset can be found in this link: https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k
            """)

st.subheader("About the models:")
st.markdown("""
            The idea behind this demo app is that we can detect fake news by comparing the topic of a given document or a news article, then compare it to the topics generated by the two BERTopic models using the two datasets. If the input topic is similar to the topics in the fake news dataset, the input is likely to be in a common fake news topic. This is by no means a perfect solution to detect fake news, but it can be a good way to quickly skim through a large number of documents and articles to find the ones that are most likely to be fake. One other advantage of topic modelling is that even though it has a lower accuracy than classification, it has a higher power in generalization. This means that it can detect fake news that are not in the training set, but are similar to the fake news in the training set.
            
            The problem with this approach is that clustering can give a general idea about the given article, but it does not give a clear answer about the article's truthfulness. For example, an article about Donald Trump or Obamacare can be detected as fake news because it is a common topic in the fake news dataset, but the content of the document itself can still be truthful. This is why this approach can be helpful in skimming through a large, unannotated set of documents, but it is important to combine this approach with other methods such as classification and fact-checking, etc. 
            
            Moreover, common topics in fake news evolve over time, so the data and models need to be updated regularly. The dataset was published in July 2022, and so far at least we have the destruction of Nord Stream pipepline as a new topic in fake news. This is why it is important to keep updating the dataset and the models down the line to keep up with the evolution of fake news.
            
            The code for the app can be found here: https://github.com/HIEUNT1610/NLP---Fake-news-detection-using-BERTopic
            """)
