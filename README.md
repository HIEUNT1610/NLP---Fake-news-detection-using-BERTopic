# NLP---Fake-news-detection-using-BERTopic
A simple demo on fake news detection using topic modelling.
            
This demo app is built to tackle the problem of fake news detection using topic modelling. The idea is that we can detect fake news by comparing the topic of a given document or a news article, and then compare it to the topics generated by the two BERTopic models using the two datasets. If the topic of the input document is similar to the topics in the fake news dataset, the input document is likely to have a common fake news topic, and therefore is likely to warrant a further assessment of veracity. This is by no means a perfect solution to detect fake news, but it can be a good way to quickly skim through a large number of documents and articles to find the ones that are most likely to be fake.
            
The backbone of the demo is BERTopic, a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.
            
The models used in the demo were trained on the Misinfo dataset from Kaggle, based on EUvsDisinfo data, using miniLM Sentence Transformer embedding. This application seeks to combine unsupervised learning techniques and labelled data to detect fake news. Two BERTopic models were used to model topics based on the fake and true news datasets. Prediction is done by generating the topic of the input and comparing it to the topics clustered by the model based on the two datasets. If the input topic is similar to the topics in the fake news dataset, the input is detected to be in common fake news topics.                      
            
At the moment, the accuracy in prediction of this demo app is not too high due to the limited hosting space from Streamlit and training data, but it is showing promise. Better and larger embedding models (such as paraphrase-multilingual-mpnet-base-v2) can give much better detection, but they go over the limit of Streamlit Community Cloud. Performance can also be further improved upon by continually adding more data to the training set. Another deployment on the Google Cloud Service will be available soon.
